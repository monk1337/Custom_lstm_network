{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "from sklearn import preprocessing \n",
    "import numpy as np\n",
    "import ast\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import operator\n",
    "import pickle as pk\n",
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "\n",
    "read_csv = pd.read_csv('tweet2.csv')\n",
    "dataset = []\n",
    "for m in range(len(read_csv)):\n",
    "    dataset.append(read_csv.iloc[m]['cleaned_tweet_decrypt_emojis'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sentiment_vector_generation():\n",
    "    \n",
    "    read_csv = pd.read_csv('tweet2.csv') #reading tweets file\n",
    "\n",
    "    def sentiment_dict():\n",
    "        sentiment_dict = {}\n",
    "\n",
    "        #reading_first_file\n",
    "        with open('NRC-Hashtag-Emotion-Lexicon-v0.2.txt','r') as f:\n",
    "            for line in tqdm_notebook(f):\n",
    "                data = line.split()\n",
    "                if data[1] not in sentiment_dict:\n",
    "                    sentiment_dict[data[1].lower()]=[float(data[2])]\n",
    "                else:\n",
    "                    sentiment_dict[data[1].lower()].append(float(data[2]))\n",
    "\n",
    "        with open('NRC-VAD-Lexicon.txt','r') as f:\n",
    "            i=0\n",
    "            for line in tqdm_notebook(f):\n",
    "                data = line.split()\n",
    "                if i>=1:\n",
    "                    try:\n",
    "                        if data[0].lower() not in sentiment_dict:\n",
    "                            sentiment_dict[data[0].lower()]= [float(data[1])]\n",
    "                        else:\n",
    "                            sentiment_dict[data[0].lower()].append(float(data[1]))\n",
    "                    except Exception:\n",
    "                        sentiment_dict[data[0].lower()]= [float(data[2])]\n",
    "\n",
    "                i+=1\n",
    "\n",
    "        #third_sentiment_file\n",
    "        with open('SemEval2015-English-Twitter-Lexicon.txt','r') as f:\n",
    "            for line in f:\n",
    "                data = line.split()\n",
    "                if data[1] not in sentiment_dict:\n",
    "                    sentiment_dict[data[1].lower()] = [float(data[0])]\n",
    "                else:\n",
    "                    sentiment_dict[data[1].lower()].append(float(data[0]))\n",
    "\n",
    "        #forth_sentinet\n",
    "        with open('senticnet5.txt','r') as f:\n",
    "            i=0\n",
    "            for line in f:\n",
    "                if i>=1:\n",
    "                    data = line.split()\n",
    "                    if data[0] not in sentiment_dict:\n",
    "                        sentiment_dict[data[0].lower()] = [float(data[2])]\n",
    "                    else:\n",
    "                        sentiment_dict[data[0].lower()].append(float(data[2]))\n",
    "                i+=1\n",
    "\n",
    "        return sentiment_dict\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    def create_vocab():\n",
    "        sentiment = []\n",
    "        tweets = []\n",
    "        vocab  = []\n",
    "        vocabs = {}\n",
    "\n",
    "        for m in range(len(read_csv)):\n",
    "            try:\n",
    "                for g in read_csv.iloc[m]['cleaned_tweet_decrypt_emojis'].split():\n",
    "                    if g not in vocab:\n",
    "                        vocab.append(g)\n",
    "                        vocabs[g] = 1\n",
    "                    else:\n",
    "                        vocabs[g] +=1\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        sorted_longs = sorted(vocabs.items(), key=operator.itemgetter(1),reverse=True)\n",
    "        sotted_list = []\n",
    "\n",
    "        for m in sorted_longs:\n",
    "            if m[1]>=2:\n",
    "                sotted_list.append(m[0])\n",
    "        with open('word_vocab_.pkl','wb') as f:\n",
    "            pk.dump(sotted_list,f)\n",
    "\n",
    "        return sotted_list\n",
    "    \n",
    "    def sentiment_vector_generation_():\n",
    "        final_vector = { 'words' : []  ,  'sentiment_vector' : [] }\n",
    "\n",
    "        sentiment_vocab_ = create_vocab()\n",
    "        sentiment_dict_  = sentiment_dict()\n",
    "\n",
    "        for m in sentiment_vocab_ :\n",
    "            if m in sentiment_dict_:\n",
    "                final_vector['words'].append(m)\n",
    "                final_vector['sentiment_vector'].append(sentiment_dict_[m])\n",
    "            else:\n",
    "                final_vector['words'].append(m)\n",
    "                final_vector['sentiment_vector'].append(np.zeros(6))\n",
    "\n",
    "        final_vector_s = { 'words' : []  ,  'sentiment_vector' : [] }\n",
    "\n",
    "        for m in zip(*(final_vector['words'],final_vector['sentiment_vector'])):\n",
    "            if len(m[1])>=6:\n",
    "                final_vector_s['words'].append(m[0])\n",
    "                final_vector_s['sentiment_vector'].append(preprocessing.normalize([m[1]],norm='l2')[0][:6].tolist())\n",
    "            else:\n",
    "                act = len(m[1])\n",
    "                hw_many = 6-len(m[1])\n",
    "                append_list = [0]*hw_many\n",
    "                m[1].extend(append_list)\n",
    "                normalize = preprocessing.normalize([m[1]],norm='l2')[0]\n",
    "                final_vector_s['words'].append(m[0])\n",
    "                final_vector_s['sentiment_vector'].append(normalize.tolist())\n",
    "                \n",
    "        save_file = pd.DataFrame(final_vector_s).to_csv('final_sentiment_vector.csv')\n",
    "        read_file_ss = pd.read_csv('final_sentiment_vector.csv')\n",
    "        \n",
    "        dict_ = {}\n",
    "        sentiment_matrix_ = []\n",
    "        \n",
    "                \n",
    "        dict_['UNK'] = [ 0.71481943,  0.98741437,  0.85255514,  0.83739983, -0.53904541, 0.87657205]\n",
    "        \n",
    "        for m in range(len(read_file_ss)):\n",
    "            dict_ [read_file_ss.iloc[m]['words']] = [float(m) for m in ast.literal_eval(read_file_ss.iloc[m]['sentiment_vector'])]\n",
    "            sentiment_matrix_.append([float(m) for m in ast.literal_eval(read_file_ss.iloc[m]['sentiment_vector'])])\n",
    "\n",
    "            \n",
    "        np.save('senti_matrix.npy',sentiment_matrix_)\n",
    "\n",
    "        return save_file\n",
    "    \n",
    "    return sentiment_vector_generation_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding_matrix(embedding_path,dim):\n",
    "    \n",
    "    #first and second vector are pad and unk words\n",
    "    \n",
    "    with open('word_vocab_.pkl','rb') as f:\n",
    "        vocab = pk.load(f)\n",
    "        \n",
    "    \n",
    "    with open(embedding_path,'r') as f:\n",
    "        word_vocab =[]\n",
    "        embedding_matrix = []\n",
    "        word_vocab.extend(['PAD','UNK'])\n",
    "        embedding_matrix.append(np.random.uniform(-1.0, 1.0, (1,dim))[0])\n",
    "        embedding_matrix.append(np.random.uniform(-1.0, 1.0, (1,dim))[0])\n",
    "\n",
    "        \n",
    "        for line in f:\n",
    "            if line.split()[0] in vocab:\n",
    "                word_vocab.append(line.split()[0])\n",
    "                embedding_matrix.append([float(i) for i in line.split()[1:]])\n",
    "                \n",
    "        np.save('word_embedding.npy',np.reshape(embedding_matrix,[-1,dim]).astype(np.float32))\n",
    "        \n",
    "        int_to_vocab = {}\n",
    "        symbols = {0: 'PAD',1: 'UNK'}\n",
    "\n",
    "\n",
    "        for index_no,word in enumerate(word_vocab):\n",
    "            int_to_vocab[index_no] = word\n",
    "            \n",
    "        int_to_vocab.update(symbols)\n",
    "        \n",
    "        print(int_to_vocab)\n",
    "        \n",
    "        vocab_to_int = {word:index_no for index_no , word in int_to_vocab.items()}\n",
    "        \n",
    "        \n",
    "        with open('int_to_vocab.pkl','wb') as f:\n",
    "            pk.dump(int_to_vocab,f)\n",
    "\n",
    "        with open('vocab_to_int.pkl','wb') as f:\n",
    "            pk.dump(vocab_to_int,f)\n",
    "            \n",
    "\n",
    "        with open('word_vocab.pkl','wb') as f:\n",
    "            pk.dump(word_vocab,f)\n",
    "            \n",
    "        \n",
    "            \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_embedding_generation():\n",
    "\n",
    "        \n",
    "    nlp = spacy.load('en')\n",
    "\n",
    "    \n",
    "    def spacy_PoS(sentence):\n",
    "        return [ w.pos_ for w in nlp(sentence) ]\n",
    "    \n",
    "    pos_vocab = []\n",
    "    for m in tqdm_notebook(dataset):\n",
    "        try:\n",
    "            pos_vocab.extend(spacy_PoS(m))\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "    \n",
    "    \n",
    "    with open('pos_vocab.pkl','wb') as f:\n",
    "        pk.dump(pos_vocab,f)\n",
    "        \n",
    "    Pos_matrix = []\n",
    "    \n",
    "    for m in range(16):\n",
    "        Pos_matrix.append(np.random.uniform(-1.0, 1.0, (1,50))[0])\n",
    "        \n",
    "    np.save('pos_matrix.npy',Pos_matrix)\n",
    "        \n",
    "    return pos_vocab\n",
    "        \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def improved_vector_generation():\n",
    "    \n",
    "    #all_vocabulary\n",
    "    \n",
    "    raw_data = pd.read_csv('tweet2.csv')\n",
    "\n",
    "    \n",
    "    nlp = spacy.load('en')\n",
    "\n",
    "    \n",
    "    def spacy_PoS(sentence):\n",
    "        return [ w.pos_ for w in nlp(sentence) ]\n",
    "\n",
    "    with open('word_vocab.pkl','rb') as f:\n",
    "        word_vocab = pk.load(f)\n",
    "\n",
    "\n",
    "\n",
    "    word_to_int = {n: m  for m,n in enumerate(word_vocab)}\n",
    "    int_to_word = {m:n  for m,n in enumerate(word_vocab)}\n",
    "\n",
    "\n",
    "    with open('word_vocab.pkl','rb') as f:\n",
    "        sentiment_vocab = pk.load(f)\n",
    "\n",
    "    senti_to_int = {n: m  for m,n in enumerate(sentiment_vocab)}\n",
    "    int_to_senti = {m:n  for m,n in enumerate(sentiment_vocab)}\n",
    "\n",
    "\n",
    "\n",
    "    with open('pos_vocab.pkl','rb') as f:\n",
    "        pos_vocab = pk.load(f)\n",
    "\n",
    "    final_tags = list(set(pos_vocab))\n",
    "\n",
    "    pos_to_int = {n: m  for m,n in enumerate(final_tags)}\n",
    "    int_to_pos = {m:n  for m,n in enumerate(final_tags)}\n",
    "\n",
    "\n",
    "    labels_vocab =  {'positive':0  ,'negative':1 , 'neutral' : 2 }\n",
    "    reverse_vocab = {0: 'positive' ,1:'negative' , 2: 'neutral' }\n",
    "    \n",
    "    \n",
    "    #all_embedding_matrix\n",
    "\n",
    "    word_embedding_matrix  = np.load('word_embedding.npy')  #300 dim\n",
    "\n",
    "    pos_embedding_matrix   = np.load('pos_matrix.npy')      # 50 dim\n",
    "\n",
    "    senti_embedding_matrix = np.load('senti_matrix.npy')    #6 dim\n",
    "    \n",
    "    \n",
    "    \n",
    "    word_embedding_encoded = []\n",
    "    pos_encoded_           = []\n",
    "    sentiment_encodeds      = []\n",
    "    labels_encoded_        = []\n",
    "    \n",
    "    def padd_local(seq_,le_n):\n",
    "        max_count = le_n - len(seq_) \n",
    "        pad_count = [0] * max_count\n",
    "        return seq_ + pad_count\n",
    "\n",
    "    for m in tqdm_notebook(range(10)):\n",
    "\n",
    "        try:\n",
    "            tokensize_normal    = raw_data.iloc[m]['cleaned_tweet_decrypt_emojis'].split()\n",
    "            pos_tags            = spacy_PoS(raw_data.iloc[m]['cleaned_tweet_decrypt_emojis'])\n",
    "            sentiment_encoded   = raw_data.iloc[m]['cleaned_tweet_decrypt_emojis'].split()\n",
    "            labels_encoded_.append(labels_vocab[raw_data.iloc[m]['airline_sentiment']])\n",
    "\n",
    "\n",
    "            new_sen =[]\n",
    "            new_pos =[]\n",
    "            new_seni=[]\n",
    "            for m in tokensize_normal:\n",
    "                if m.lower() in word_vocab:\n",
    "                    new_sen.append(word_to_int[m.lower()])\n",
    "                else:\n",
    "                    new_sen.append(word_to_int['UNK'])\n",
    "\n",
    "            for k in pos_tags:\n",
    "                new_pos.append(pos_to_int[k])\n",
    "                \n",
    "\n",
    "            for j in sentiment_encoded:\n",
    "                if j in sentiment_vocab:\n",
    "                    new_seni.append(senti_to_int[j])\n",
    "                else:\n",
    "                    new_seni.append(senti_to_int['UNK'])\n",
    "\n",
    "            word_embedding_encoded.append(new_sen)\n",
    "            pos_encoded_.append(new_pos)\n",
    "            sentiment_encodeds.append(new_seni)\n",
    "        except Exception:\n",
    "            pass\n",
    "        \n",
    "#         print(pos_to_int)\n",
    "        \n",
    "        word_embedd_lookup = []\n",
    "        pos_embedd_lookup  =[]\n",
    "        senti_embedd_lookup =[]\n",
    "        Improved_vector = []\n",
    "        \n",
    "        for word_ in zip(word_embedding_encoded,pos_encoded_,sentiment_encodeds):\n",
    "            max_len = max([len(word_[0]),len(word_[1]),len(word_[2])])\n",
    "            \n",
    "            word_vector = padd_local(word_[0],max_len)\n",
    "            pos_vector  = padd_local(word_[1],max_len)\n",
    "            senti_vector = padd_local(word_[2],max_len)\n",
    "            \n",
    "            word_look =[]\n",
    "            pos_look  =[]\n",
    "            senti_look =[]\n",
    "            \n",
    "            for local_word in word_vector:\n",
    "                word_look.append(word_embedding_matrix[local_word])\n",
    "            for local_pos in pos_vector:\n",
    "                pos_look.append(pos_embedding_matrix[local_pos])\n",
    "            for local_senti in senti_vector:\n",
    "                senti_look.append(senti_embedding_matrix[local_senti])\n",
    "                \n",
    "            Improved_vector.append(np.column_stack((word_look,pos_look,senti_look)))\n",
    "            \n",
    "            print(np.column_stack((word_look,pos_look,senti_look)).shape)\n",
    "            \n",
    "        return Improved_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-5feaaf5b5227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msentiment_vector_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mword_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./glove.6B.300d.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpos_embedding_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimproved_vector_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-631d9009bdcd>\u001b[0m in \u001b[0;36msentiment_vector_generation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msave_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msentiment_vector_generation_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-631d9009bdcd>\u001b[0m in \u001b[0;36msentiment_vector_generation_\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mfinal_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m \u001b[0;34m'words'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m,\u001b[0m  \u001b[0;34m'sentiment_vector'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0msentiment_vocab_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0msentiment_dict_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msentiment_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-631d9009bdcd>\u001b[0m in \u001b[0;36mcreate_vocab\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'cleaned_tweet_decrypt_emojis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m                         \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                         \u001b[0mvocabs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sentiment_vector_generation()\n",
    "word_embedding_matrix('./glove.6B.300d.txt',300)\n",
    "pos_embedding_generation()\n",
    "improved_vector_generation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
